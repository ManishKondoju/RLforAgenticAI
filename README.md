# RL for Agentic AI â€“ Smart Triage RL System

## ğŸ“Œ Project Overview
This project implements a **Reinforcement Learning (RL) system** for intelligent task triage and scheduling.  
It leverages **DQN, Contextual Bandits, and PPO** to optimize task resolution and minimize violations in a simulated environment.

The system is designed to simulate **agentic AI decision-making** in environments requiring priority-based task handling.

---

## ğŸš€ Features
- **Multi-Algorithm RL**: DQN, Contextual Bandits, and PPO integration.
- **Performance Tracking**: Compare baseline vs trained models.
- **Statistical Validation**: Paired t-tests to verify improvements.
- **Interactive Visualization**: Learning curves, comparative bar plots, and agent performance analysis.
- **Scalable Design**: Extendable to multi-agent setups and real-world scheduling.

---

## ğŸ“‚ Project Structure
```
smart-triage-rl/
â”‚â”€â”€ env/                   # Custom RL environment
â”‚â”€â”€ agents/                # RL algorithms (DQN, PPO, Bandit)
â”‚â”€â”€ eval/                  # Evaluation scripts and plots
â”‚â”€â”€ models/                # Saved trained models
â”‚â”€â”€ plots/                 # Generated performance plots
â”‚â”€â”€ dash/                  # Streamlit dashboard for visualization
â”‚â”€â”€ main.py                # Training + evaluation script
â”‚â”€â”€ requirements.txt       # Dependencies
â”‚â”€â”€ README.md              # Project documentation
```
---

## âš™ï¸ Installation
```bash
git clone https://github.com/ManishKondoju/RLforAgenticAI.git
cd RLforAgenticAI
pip install -r requirements.txt
```

---

## â–¶ï¸ Usage

### **1. Train & Evaluate Models**
```bash
python main.py
```

### **2. Launch Visualization Dashboard**
```bash
streamlit run dash/app.py
```

---

## ğŸ“Š Experimental Methodology
1. **Training Phase**: Train each agent (Baseline, DQN, DQN+Bandit, DQN+Bandit+PPO).
2. **Evaluation Phase**: Run episodes, collect rewards, resolutions, and violations.
3. **Analysis Phase**: Statistical tests and visualizations.

---

## ğŸ“ˆ Results Summary

| Variant              | Avg Reward | Resolved Tasks | Violations |
|----------------------|------------|---------------|------------|
| **Baseline**         | ~157       | 66.00          | 0.59       |
| **DQN**              | ~346       | 199.25         | 0.04       |
| **DQN + Bandit**     | ~347       | 199.99         | 0.09       |
| **DQN + Bandit + PPO** | ~360     | 200.00         | 0.09       |

âœ… **DQN+Bandit+PPO** showed the highest improvement in reward and resolution rate.

---

## ğŸ§  RL Techniques Used
- **DQN** for value-based learning.
- **Contextual Bandit** for adaptive task selection.
- **PPO** for policy-based scheduling optimization.

---

## ğŸ”® Future Improvements
- Multi-agent collaboration.
- Human-in-the-loop feedback.
- More contextual features in Bandit models.
- Real-world deployment.

---

## âš– Ethical Considerations
- Fair task allocation.
- Transparency and explainability.
- Data privacy compliance.
- Avoidance of bias.

---

## ğŸ“œ License
This project is licensed under the MIT License.
